{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codifica tus própios perceptrones para implementar puertas lógicas\n",
    "En las partes que aparecen así\n",
    "```python\n",
    "pass  # ⬅️✏️\n",
    "```\n",
    "necesitas rellenar código antes de pasar a la siguiente celda.\n",
    "\n",
    "Revisa las transparencias de clase para llevar a cabo estos ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comenzaremos por implementar una **neurona AND**. Ojo!! No la vamos a entrenar, vamos a asumir que conocemos los pesos (los hemos calculado en clase) \n",
    "\n",
    "Para ello:\n",
    "\n",
    "1) suponemos que el entrenamiento ya está previamente hecho y por lo tanto conocemos los pesos apropiados (consultar las transparencias)\n",
    "\n",
    "2) Nos piden implementar la neurona AND y probar con un item o ejemplo, por ejemplo un vector de input 0,1 que la salida es correcta\n",
    "\n",
    "Recordad que en clase hemos descubierto que los pesos apropiados son:\n",
    "0.66 y 0.8, así que el vector de pesos será [0.66,0.8] y el bias será 1 y el peso para el bias será -0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Definir dos vectores (listas): input my_x, pesos my_w\n",
    "my_x = [0, 1]#input un item\n",
    "my_w = [0.66, 0.80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicar dos vectores elemento a elemento\n",
    "def mul(a, b):\n",
    "    \"\"\"\n",
    "    devolver una lista c, de la misma longitud que a y b donde \n",
    "    cada elemento c[i] = a[i] * b[i]\n",
    "    lo podéis hacer con un bucle o con una list comprenhension\n",
    "    \"\"\"\n",
    "    pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.8]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test la función mul() con un item my_x \n",
    "# y los pesos descubiertos en clase my_w, el resultado debería ser \n",
    "# el vector [0.0,0.8]\n",
    "mul(my_x, my_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97, 0.66, 0.8]\n"
     ]
    }
   ],
   "source": [
    "# Define el bias my_bias y el peso descubierto en clase asociado a ese bias\n",
    "# añadiré el bias a el vector de pesos my_w generando un nuevo vector my_wPlusWBias.\n",
    "# Posibles errores: Recordad que en Python las variables con punteros\n",
    "# y el insertar si lo ejecutáis varias veces los valores \n",
    "# se van acumulando dependiendo de cómo hagáis la inserción\n",
    "# my_wPlusWBias debería contener [-0.97, 0.66, 0.8]. Pista para hacer copias de un vector. copiaV=v[:] o copiaV=v.copy()\n",
    "\n",
    "my_bias  = 1\n",
    "my_wbias = -0.97\n",
    "\n",
    "pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurona lineal\n",
    "def distanciaDelCoseno(x, weights, bias):\n",
    "    \"\"\"\n",
    "    El producto escalar (producto punto) de dos vectores y la similitud de coseno no son completamente equivalentes \n",
    "    ya que la similitud del coseno solo se preocupa por la diferencia de ángulo, \n",
    "    mientras que el producto de punto se preocupa por el ángulo y la magnitud\n",
    "    Pero en muchas ocasiones se emplean indistintamente\n",
    "    Así pues, esta función devuelve el valor escalar de la neurona, es decir, \n",
    "    el producto escalar entre el vector de entrada añadiendo el bias y el vector de los pesos\n",
    "    recordad que \"sum(list)\" computa la suma de los elementos de una lista\n",
    "    Así pues se comenzará por añadir el bías en la posición 0 del vector de entrada \n",
    "    antes de llevar a cabo el producto escalar para así tener dos vectores de \n",
    "    la misma longitud. Emplea la función mul que ya has programado\n",
    "    \"\"\"\n",
    "    pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16999999999999993"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test distanciaDelCoseno que debería darte -0.16999999999999993 para los datos my_x, my_wPlusWBias, my_bias\n",
    "distanciaDelCoseno(my_x, my_wPlusWBias, my_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una neurona perceptron completa, distancia del coseno y activación\n",
    "def neuron(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Devolverá el output de una neurona clásica \n",
    "    (reutilizar la distancia del coseno definida previamente) \n",
    "    y añadir la función de activación (step function): si >=0 entonces 1 sino -1\n",
    "    \"\"\"\n",
    "    output=-1\n",
    "    pass  # ⬅️✏️\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testar la función neuron() para el item my_x y el bias my_b \n",
    "# y el vector de pesos my_wPlusWBias\n",
    "# debería de dar -1 para el input item [0,1] con el bias 1 \n",
    "# y el vector de pesos hayado en clase\n",
    "neuron(my_x, my_wPlusWBias, my_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Package AND neuron weights and bias\n",
    "def and_neuron(x):\n",
    "    \"\"\"\n",
    "    Devuelve x1 AND x2 suponiendo que la hemos entrenado\n",
    "    y que en ese entrenamiento hemos aprendido los pesos apropiados \n",
    "    (mirar las transparencias de clase). Así pues inicializaremos \n",
    "    una la variable local and_w con los pesos aprendidos \n",
    "    y a 1 la variable local and_bias \n",
    "    y ejecutaremos la función neurona para el item x\"\"\"\n",
    "    and_w    = [-0.97,0.66, 0.80]#initialization of the weights and_w\n",
    "    and_bias = 1#initialization of the bias and_bias\n",
    "    pass  # llamar a la función generica neurona con los pesos de una puerta AND⬅️✏️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nos piden probar la puerta para toda la colección de inputs posibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los items de entrada para testar\n",
    "# las neuronas AND y las posteriores que implementaremos (OR, XOR)\n",
    "# CUIDADO para la neurona NOT hará falta otra colección dado \n",
    "# que los vectores de entrada a la NOT no tienen dos dimensiones sino 1\n",
    "my_x_collection = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando el output de la neurona AND\n",
      "[0, 0] -1.000\n",
      "[0, 1] -1.000\n",
      "[1, 0] -1.000\n",
      "[1, 1] 1.000\n"
     ]
    }
   ],
   "source": [
    "# Para los items de entrada my_x_collection la salida debería ser \n",
    "# -1, -1, -1, 1\n",
    "print('Testando el output de la neurona AND')\n",
    "#bucle para ir obteniendo el output de la neurona AND para cada item del input\n",
    "for my_x in my_x_collection:\n",
    "    print(my_x, f'{and_neuron(my_x):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona OR\n",
    "\n",
    "Hasta ahora solo habéis tenido que implementar la neurona AND sin tener que entrenarla dado que ya conocíamos los pesos que habíamos aprendido en clase. Es decir, no habéis implementado en Python la fase de entrenamiento de la neurona para determinar los pesos. Ahora se os pide que entrenéis una neurona OR, de forma que realicéis iteraciones sobre los items del input. Para ello los pasos serán:\n",
    "1) Inicializar un vector de pesos de forma random (emplear la librería random **from random import random**)\n",
    "\n",
    "2) Por cada item del input aplicar la neurona y si la predicción realizada por la neurona en base a aplicar  la distancia del coseno y la función de activación no es correcta, entonces ajustar los pesos consecuentemente\n",
    "\n",
    "3) Repetir el paso 2 hasta convergencia (es decir, hasta que todos los items estén correctamente clasificados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando una neurona OR hasta convergencia\n",
      "Imprimiendo los pesos random [0.13436424411240122, 0.8474337369372327, 0.763774618976614] \n",
      "\n",
      "\n",
      "### Estamos en la vuelta (epoch) 1\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold -1\n",
      "  Se han cambiado los pesos\n",
      "[-0.8656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:-1 Gold 1\n",
      "  Se han cambiado los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 2\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold -1\n",
      "  Se han cambiado los pesos\n",
      "[-0.8656357558875988, 0.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 0.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 0.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 0.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:-1 Gold 1\n",
      "  Se han cambiado los pesos\n",
      "[0.13436424411240122, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[0.13436424411240122, 1.8474337369372327, 1.7637746189766141]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 3\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold -1\n",
      "  Se han cambiado los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 4\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:-1 Gold -1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.8656357558875988, 1.8474337369372327, 1.7637746189766141]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "#inicializaciones\n",
    "print('Entrenando una neurona OR hasta convergencia')\n",
    "notConverge=True\n",
    "seed(1)# Si queremos que el proceso de inicialización random sea replicable\n",
    "orWeights= [random() for i in range(3)]#inicializar de forma random el vector de pesos or_weights\n",
    "print(\"Imprimiendo los pesos random\", orWeights, \"\\n\")\n",
    "orBias   = 1#inicialización del bias a 1\n",
    "orGoldOutputs=[-1,1,1,1]#inicialización del Gold Standard o patrón oro, \n",
    "# es decir, el output que la neurona OR debería aprender a obtener\n",
    "\n",
    "#entrenando\n",
    "numeroVuelta = 0\n",
    "#while notConverge:\n",
    " pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurona NOT\n",
    "\n",
    "Ahora implementa el entrenamiento de una neurona NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los items de entrada para testar\n",
    "# la neurona NOT. \n",
    "# Recordad que los vectores de entrada a la NOT no tienen dos dimensiones sino 1\n",
    "my_x_collection = [\n",
    "    [0],\n",
    "    [1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando una neurona NOT hasta convergencia\n",
      "Imprimiendo los pesos random [0.13436424411240122, 0.8474337369372327] \n",
      "\n",
      "\n",
      "### Estamos en la vuelta (epoch) 1\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327]\n",
      "predictedOutput:1 Gold 1\n",
      "[0.13436424411240122, 0.8474337369372327]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327]\n",
      "predictedOutput:1 Gold -1\n",
      "  Se han cambiado los pesos\n",
      "[-0.8656357558875988, -0.15256626306276733]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 2\n",
      "Imprimiendo los pesos\n",
      "[-0.8656357558875988, -0.15256626306276733]\n",
      "predictedOutput:-1 Gold 1\n",
      "  Se han cambiado los pesos\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "predictedOutput:-1 Gold -1\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 3\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "predictedOutput:1 Gold 1\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, -0.15256626306276733]\n",
      "predictedOutput:-1 Gold -1\n",
      "[0.13436424411240122, -0.15256626306276733]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "\n",
    "#inicializaciones\n",
    "print('Entrenando una neurona NOT hasta convergencia')\n",
    "notConverge=True\n",
    "seed(1)# Si queremos que el proceso de inicialización random sea replicable\n",
    "#inicializar de forma random el vector de pesos notWeights\n",
    "pass  # ⬅️✏️\n",
    "print(\"Imprimiendo los pesos random\", notWeights, \"\\n\")\n",
    "notBias   = 1#inicialización del bias a 1\n",
    "#inicialización del Gold Standard o patrón oro,notGoldOutput. CUIDADO con el número de valores que ponéis \n",
    "# es decir, el output que la neurona OR debería aprender a obtener\n",
    "\n",
    "#entrenando\n",
    "numeroVuelta = 0\n",
    "#while notConverge:\n",
    " pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average\n",
    "\n",
    "Ahora implementa el weighted average explicado en las transparencias de clase ¿qué puedes decir acerca de las actualizaciones de los pesos y el número de epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen los items de entrada para testar\n",
    "# las neuronas AND y las posteriores que implementaremos (OR, XOR)\n",
    "# CUIDADO para la neurona NOT hará falta otra colección dado \n",
    "# que los vectores de entrada a la NOT no tienen dos dimensiones sino 1\n",
    "my_x_collection = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixAverage(m):\n",
    "    res=list()\n",
    "    acum=list()\n",
    "    if len(m) > 0:\n",
    "        res=[0]*len(m[0])\n",
    "        for v in m:\n",
    "            res = [a+b for a,b in zip (res,v)]\n",
    "        acum=[elem/len(m) for elem in res]\n",
    "    return acum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[4, 6, 8]\n",
      "[6, 9, 12]\n",
      "acum\n",
      "[2.0, 3.0, 4.0]\n",
      "[2.0, 3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "matrix=[[2,3,4],[2,3,4],[2,3,4]]\n",
    "print(matrixAverage(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando una neurona OR hasta convergencia\n",
      "Imprimiendo los pesos random [0.13436424411240122, 0.8474337369372327, 0.763774618976614] \n",
      "\n",
      "\n",
      "### Estamos en la vuelta (epoch) 1\n",
      "Imprimiendo los pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold -1\n",
      "AuxAct\n",
      "[-0.8656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "  Se han añadido pesos\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "[-0.7312715117751976, 1.6948674738744653, 1.527549237953228]\n",
      "acum\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "[-0.7312715117751976, 1.6948674738744653, 1.527549237953228]\n",
      "acum\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 2\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:-1 Gold -1\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "Imprimiendo los pesos\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n",
      "predictedOutput:1 Gold 1\n",
      "[-0.3656357558875988, 0.8474337369372327, 0.763774618976614]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "#inicializaciones\n",
    "print('Entrenando una neurona OR hasta convergencia')\n",
    "notConverge=True\n",
    "seed(1)# Si queremos que el proceso de inicialización random sea replicable\n",
    "orWeights= [random() for i in range(3)]#inicializar de forma random el vector de pesos or_weights\n",
    "print(\"Imprimiendo los pesos random\", orWeights, \"\\n\")\n",
    "orBias   = 1#inicialización del bias a 1\n",
    "orGoldOutputs=[-1,1,1,1]#inicialización del Gold Standard o patrón oro, \n",
    "# es decir, el output que la neurona OR debería aprender a obtener\n",
    "weightLength       = len(orWeights) \n",
    "#entrenando\n",
    "numeroVuelta = 0\n",
    "#while notConverge:\n",
    "    pass  # ⬅️✏️\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package OR neuron weights and bias\n",
    "def or_neuron(x):\n",
    "    \"\"\"\n",
    "    Devuelve x1 AND x2 suponiendo que la hemos entrenado\n",
    "    y que en ese entrenamiento hemos aprendido los pesos apropiados \n",
    "    (mirar las transparencias de clase). Así pues inicializaremos \n",
    "    una la variable local and_w con los pesos aprendidos \n",
    "    y a 1 la variable local and_bias \n",
    "    y ejecutaremos la función neurona para el item x\"\"\"\n",
    "    or_w    = [-0.3656,0.8474, 0.7637]#initialization of the weights and_w\n",
    "    or_bias = 1#initialization of the bias and_bias\n",
    "    pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando el output de la neurona OR\n",
      "[0, 0] -1.000\n",
      "[0, 1] 1.000\n",
      "[1, 0] 1.000\n",
      "[1, 1] 1.000\n"
     ]
    }
   ],
   "source": [
    "# Para los items de entrada my_x_collection la salida debería ser \n",
    "# -1, -1, -1, 1\n",
    "print('Testando el output de la neurona OR')\n",
    "#bucle para ir obteniendo el output de la neurona AND para cada item del input\n",
    "for my_x in my_x_collection:\n",
    "    print(my_x, f'{or_neuron(my_x):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![X-OR](res/xorToLearnWeights.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando una neurona XOR hasta convergencia\n",
      "Imprimiendo los pesos random [0.13436424411240122, 0.8474337369372327, 0.763774618976614] \n",
      "\n",
      "\n",
      "### Estamos en la vuelta (epoch) 1\n",
      "[0, 0]\n",
      "[-1, -1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, -1, -1]\n",
      "0.13436424411240122 -0.5\n",
      "-0.3656357558875988\n",
      "0.8474337369372327 -1\n",
      "-0.15256626306276733\n",
      "0.763774618976614 -1\n",
      "-0.23622538102338597\n",
      "AuxAct\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[0.13436424411240122, 0.8474337369372327, 0.763774618976614], [-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]]\n",
      "####\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "[-0.23127151177519756, 0.6948674738744653, 0.5275492379532281]\n",
      "acum\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "####\n",
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614]\n",
      "[-0.23127151177519756, 0.6948674738744653, 0.5275492379532281]\n",
      "acum\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "[0, 1]\n",
      "[-1, 1]\n",
      "[1, 0]\n",
      "[-1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 2\n",
      "[0, 0]\n",
      "[-1, -1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, -1, -1]\n",
      "-0.11563575588759878 -0.5\n",
      "-0.6156357558875988\n",
      "0.3474337369372327 -1\n",
      "-0.6525662630627673\n",
      "0.26377461897661403 -1\n",
      "-0.736225381023386\n",
      "AuxAct\n",
      "[-0.6156357558875988, -0.6525662630627673, -0.736225381023386]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403], [-0.6156357558875988, -0.6525662630627673, -0.736225381023386]]\n",
      "####\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "[-0.7312715117751976, -0.30513252612553465, -0.47245076204677194]\n",
      "acum\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "####\n",
      "[-0.11563575588759878, 0.3474337369372327, 0.26377461897661403]\n",
      "[-0.7312715117751976, -0.30513252612553465, -0.47245076204677194]\n",
      "acum\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "[0, 1]\n",
      "[-1, 1]\n",
      "predictedOutput:1 Gold -1\n",
      "[-0.5, -1, 1]\n",
      "-0.3656357558875988 -0.5\n",
      "0.13436424411240122\n",
      "-0.15256626306276733 -1\n",
      "0.8474337369372327\n",
      "-0.23622538102338597 1\n",
      "-1.2362253810233859\n",
      "AuxAct\n",
      "[0.13436424411240122, 0.8474337369372327, -1.2362253810233859]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597], [0.13436424411240122, 0.8474337369372327, -1.2362253810233859]]\n",
      "####\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "[-0.23127151177519756, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859]\n",
      "[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859]\n",
      "####\n",
      "[-0.3656357558875988, -0.15256626306276733, -0.23622538102338597]\n",
      "[-0.23127151177519756, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859]\n",
      "[1, 0]\n",
      "[-1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, 1, 1]\n",
      "-0.11563575588759878 -0.5\n",
      "-0.6156357558875988\n",
      "0.3474337369372327 1\n",
      "1.3474337369372327\n",
      "-0.7362253810233859 1\n",
      "0.26377461897661414\n",
      "AuxAct\n",
      "[-0.6156357558875988, 1.3474337369372327, 0.26377461897661414]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859], [-0.6156357558875988, 1.3474337369372327, 0.26377461897661414]]\n",
      "####\n",
      "[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859]\n",
      "[-0.7312715117751976, 1.6948674738744653, -0.4724507620467717]\n",
      "acum\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "####\n",
      "[-0.11563575588759878, 0.3474337369372327, -0.7362253810233859]\n",
      "[-0.7312715117751976, 1.6948674738744653, -0.4724507620467717]\n",
      "acum\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 3\n",
      "[0, 0]\n",
      "[-1, -1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, -1, -1]\n",
      "-0.3656357558875988 -0.5\n",
      "-0.8656357558875988\n",
      "0.8474337369372327 -1\n",
      "-0.15256626306276733\n",
      "-0.23622538102338586 -1\n",
      "-1.2362253810233859\n",
      "AuxAct\n",
      "[-0.8656357558875988, -0.15256626306276733, -1.2362253810233859]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586], [-0.8656357558875988, -0.15256626306276733, -1.2362253810233859]]\n",
      "####\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "[-1.2312715117751976, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859]\n",
      "[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859]\n",
      "####\n",
      "[-0.3656357558875988, 0.8474337369372327, -0.23622538102338586]\n",
      "[-1.2312715117751976, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859]\n",
      "[0, 1]\n",
      "[-1, 1]\n",
      "[1, 0]\n",
      "[-1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, 1, 1]\n",
      "-0.6156357558875988 -0.5\n",
      "-1.115635755887599\n",
      "0.3474337369372327 1\n",
      "1.3474337369372327\n",
      "-0.7362253810233859 1\n",
      "0.26377461897661414\n",
      "AuxAct\n",
      "[-1.115635755887599, 1.3474337369372327, 0.26377461897661414]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859], [-1.115635755887599, 1.3474337369372327, 0.26377461897661414]]\n",
      "####\n",
      "[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859]\n",
      "[-1.7312715117751978, 1.6948674738744653, -0.4724507620467717]\n",
      "acum\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "####\n",
      "[-0.6156357558875988, 0.3474337369372327, -0.7362253810233859]\n",
      "[-1.7312715117751978, 1.6948674738744653, -0.4724507620467717]\n",
      "acum\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 4\n",
      "[0, 0]\n",
      "[-1, -1]\n",
      "predictedOutput:-1 Gold 1\n",
      "[-0.5, -1, -1]\n",
      "-0.8656357558875989 -0.5\n",
      "-1.365635755887599\n",
      "0.8474337369372327 -1\n",
      "-0.15256626306276733\n",
      "-0.23622538102338586 -1\n",
      "-1.2362253810233859\n",
      "AuxAct\n",
      "[-1.365635755887599, -0.15256626306276733, -1.2362253810233859]\n",
      "  Se han añadido pesos\n",
      "#### matrix Average\n",
      "[[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586], [-1.365635755887599, -0.15256626306276733, -1.2362253810233859]]\n",
      "####\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "[-2.231271511775198, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
      "####\n",
      "[-0.8656357558875989, 0.8474337369372327, -0.23622538102338586]\n",
      "[-2.231271511775198, 0.6948674738744653, -1.4724507620467717]\n",
      "acum\n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
      "[0, 1]\n",
      "[-1, 1]\n",
      "[1, 0]\n",
      "[-1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
      "\n",
      "### Estamos en la vuelta (epoch) 5\n",
      "[0, 0]\n",
      "[-1, -1]\n",
      "[0, 1]\n",
      "[-1, 1]\n",
      "[1, 0]\n",
      "[-1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
      "Se ha producido convergencia el los siguientes valores: \n",
      "[-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n"
     ]
    }
   ],
   "source": [
    "# Combinando una puerta OR y una AND, y aprendiendo el peso que hay que darle a cada una para obtener un XOR \n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "#inicializaciones\n",
    "print('Entrenando una neurona XOR hasta convergencia')\n",
    "xorConverge=True\n",
    "seed(1)# Si queremos que el proceso de inicialización random sea replicable\n",
    "xorWeights= [random() for i in range(3)]#inicializar de forma random el vector de pesos or_weights\n",
    "print(\"Imprimiendo los pesos random\", xorWeights, \"\\n\")\n",
    "xorBias   = -0.5#inicialización del bias a 0.5\n",
    "xorGoldOutputs=[1,-1,-1,1]#inicialización del Gold Standard o patrón oro, \n",
    "# es decir, el output que la red XOR debería aprender a obtener\n",
    "#entrenando\n",
    "numeroVuelta = 0\n",
    "\n",
    "#while xorConverge:\n",
    "  pass  # ⬅️✏️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_neuron(x):\n",
    "    \"\"\"\n",
    "    Return x1_ * x2 + x1 * x2_\n",
    "    \"\"\"\n",
    "    xor_w    = [-1.115635755887599, 0.3474337369372327, -0.7362253810233859]\n",
    "    xor_bias = -0.5\n",
    "    new_x=list()\n",
    "    new_x.append(and_neuron(x))\n",
    "    new_x.append(or_neuron(x))\n",
    "    return neuron(new_x, xor_w, xor_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking XOR neuron output\n",
      "[0, 0] 1.000\n",
      "[0, 1] -1.000\n",
      "[1, 0] -1.000\n",
      "[1, 1] 1.000\n"
     ]
    }
   ],
   "source": [
    "print('Checking XOR neuron output')\n",
    "for my_x in my_x_collection:\n",
    "    print(my_x, f'{xor_neuron(my_x):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
